{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Install BioTex\n",
    "#!git clone https://gitlab.irstea.fr/jacques.fize/biotex_python.git\n",
    "#cd biotex_python\n",
    "#sudo pip3 install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms Extx Pkgs\n",
    "from biotex import BiotexWrapper\n",
    "#EDA Pkgs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###extract text for BioTex input\n",
    "def convert_pdf_to_txt(src_file_path):\n",
    "    \"\"\"\n",
    "        Appel externe à pdftotext.\n",
    "        -q : pas de message d'erreur dans la sortie.\n",
    "         - : envoie la sortie dans la console au lieu d'un fichier texte.\n",
    "\n",
    "        Capture de la sortie texte.\n",
    "\n",
    "        @type  src_file_path: String.\n",
    "        @param src_file_path: Chemin du fichier source.\n",
    "\n",
    "        @rtype: String.\n",
    "        @return: Texte brut.\n",
    "    \"\"\"\n",
    "    completed_process = subprocess.run([\"pdftotext\", \"-q\", src_file_path, \"-\"], stdout=subprocess.PIPE)\n",
    "    return completed_process.stdout.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean docs\n",
    "def cleanhtml(raw_text, remove_punc=False, lower=False):\n",
    "    \"\"\"\n",
    "    Replace HTML tags in a text.\n",
    "\n",
    "    raw_html : str\n",
    "        html in its raw form\n",
    "    \"\"\"\n",
    "    clean_text = raw_text\n",
    "\n",
    "    # Remove hmtl and url patterns\n",
    "    patterns = [re.compile('<.*?>'), re.compile('\\[\\d\\]'), re.compile('www.\\S+.com')]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        clean_text = re.sub(pattern, '', clean_text)\n",
    "\n",
    "    # Special characters causing pb with Biotex\n",
    "    # ['\\n', '\\t', 'ã', '€', \"\\'\", \"\\xa0\"]\n",
    "    toRemove = ['\\n', '\\t','\\\"', 'ã', '€', \"\\xa0\"]\n",
    "\n",
    "    for char in toRemove:\n",
    "        clean_text = re.sub(char, '', clean_text)\n",
    "\n",
    "    # add whitespace after a dot\n",
    "    rx = r\"\\.(?=\\S)\"\n",
    "    clean_text = re.sub(rx, \". \", clean_text)\n",
    "\n",
    "    if remove_punc:\n",
    "        clean_text = re.sub('[^A-Za-z0-9]+', ' ', clean_text)\n",
    "\n",
    "    if lower:\n",
    "        clean_text = clean_text.lower()\n",
    "\n",
    "    return clean_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_file_path = \"/home/rodrique/Bureau/Jupyter-notebook/herelles/corpus_experts/Urbanisme/Etude-urbaine-et-paysagere-A9-deplacee.pdf\"\n",
    "# convert_pdf_to_txt(src_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urb = \"./corpus_experts/Urbanisme/*.pdf\"\n",
    "urb_file_list = glob.glob(urb) # Include slash or it will search in the wrong directory!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risq = \"./corpus_experts/Risques naturels/*.pdf\"\n",
    "risq_file_list = glob.glob(risq) # Include slash or it will search in the wrong directory!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "urb_doc = {}\n",
    "risq_doc = {}\n",
    "for doc in urb_file_list:\n",
    "    fnamesrc = Path(doc).stem\n",
    "    print(fnamesrc)\n",
    "    urb_doc[fnamesrc] = cleanhtml(convert_pdf_to_txt(doc))\n",
    "    with open('./corpus_experts/terms_urb/urb_docB.txt', 'a') as f:\n",
    "        f.write(\"%s\\n\" % convert_pdf_to_txt(doc))\n",
    "        f.write(\"\\n##########END##########\\n\")\n",
    "    \n",
    "for doc_ in risq_file_list:\n",
    "    fnamesrc = Path(doc_).stem\n",
    "    print(fnamesrc)\n",
    "    risq_doc[fnamesrc] = cleanhtml(convert_pdf_to_txt(doc_))\n",
    "    with open('./corpus_experts/terms_risq/risq_docB.txt', 'a') as f:\n",
    "        f.write(\"%s\\n\" % convert_pdf_to_txt(doc_))\n",
    "        f.write(\"\\n##########END##########\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urb_doc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risq_doc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extract terms with BioTEX\n",
    "def biotex_terms_extractor(corpus, language):\n",
    "    params = ['C_value','F-TFIDF-C_M']\n",
    "    for p in params:\n",
    "        wrapper_p = BiotexWrapper(language=language, score=p)\n",
    "        \n",
    "        root, filename = os.path.split(corpus)\n",
    "        filename = filename.split('.txt')[0]\n",
    "        filc = open(corpus, 'r')\n",
    "        \n",
    "        content = filc.read()\n",
    "        contentdata = [content]\n",
    "        data = wrapper_p.terminology(contentdata)\n",
    "        data.to_csv('./corpus_experts/terms_urb/'+ filename + '_' +p+ \".csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in urb_doc.keys():\n",
    "#     content = urb_doc[k]\n",
    "#     biotex_terms_extractor(content, 'french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_urb = \"./corpus_experts/terms_urb/urb_docB.txt\"\n",
    "corpus_risq = \"./corpus_experts/terms_risq/risq_docB.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biotex_terms_extractor(corpus_urb, 'french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = \"./corpus_experts/terms_risq/risq_docB_C_value.csv\"  \n",
    "# tf = \"./corpus_experts/terms_risq/risq_docB_F-TFIDF-C_M.csv\"\n",
    "\n",
    "cv = \"./corpus_experts/terms_urb/urb_docB_C_value.csv\"\n",
    "tf = \"./corpus_experts/terms_urb/urb_docB_F-TFIDF-C_M.csv\"\n",
    "df1 = pd.read_csv(cv, sep='\\t')\n",
    "df2 = pd.read_csv(tf, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[['term', 'rank']]\n",
    "df2 = df2[['term', 'rank']]\n",
    "DF = []\n",
    "DF.append(df1)\n",
    "DF.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "df = reduce(lambda df1,df2: pd.merge(df1,df2,how=\"left\", on=['term']), DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average'] = df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "final_df = df.sort_values(by=['average'], ascending=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./corpus_experts/terms_urb/final_terms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read kwds list\n",
    "def read_kwd(txtfile):\n",
    "    f = open(txtfile)\n",
    "    content = f.read()\n",
    "    kw_list = (content.lower()).split('\\n')\n",
    "    return kw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms semantic similarity evaluation\n",
    "def eval_terms(corpus,queries,top_k):\n",
    "    embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    D = {}\n",
    "    for query in queries:\n",
    "        D[query] = []\n",
    "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        cos_scores = cos_scores.cpu()\n",
    "\n",
    "        #We use np.argpartition, to only partially sort the top_k results\n",
    "        top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "        for idx in top_results[0:top_k]:\n",
    "            D[query].append( float('%.4f' % (cos_scores[idx])) )\n",
    "    for val_ in D:\n",
    "        D[val_]= sum(D[val_])/len(D[val_])\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for similarity measure\n",
    "def core_biotT_and_expertT(t_file, kwd_file, n_first):\n",
    "    d_biotex = {}\n",
    "#     files = glob.glob(t_file + '/*mean.csv',recursive = True) \n",
    "    root, filenam = os.path.split(t_file)\n",
    "    filenam = filenam.split('.csv')[0]\n",
    "\n",
    "    kw_list = read_kwd(kwd_file)\n",
    "    kw_list = kw_list[:-1]\n",
    "\n",
    "    terms = pd.read_csv(t_file, sep='\\t|,', engine='python')\n",
    "\n",
    "    df = terms['term']\n",
    "    df = np.array(df)\n",
    "    \n",
    "    df = [word for word in df if (word not in stopwords.words('french'))]\n",
    "    df = [word for word in df if word]\n",
    "    d_bert = eval_terms(kw_list ,df[:1000],n_first)\n",
    "    \n",
    "    df = pd.DataFrame(list(d_bert.items()), columns=['term','rank'])\n",
    "    final_df = df.sort_values(by=['rank'], ascending=False)\n",
    "\n",
    "    final_df.to_csv(root+'/'+filenam+'_Bert.csv',index= False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_biotT_and_expertT(t_file, kwd_file, n_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biotex_out = \"./corpus_experts/terms_risq/final_terms.csv\"\n",
    "# expert_concept = './Herelles_ress/termes_graines_natural.dangers.txt'  \n",
    "\n",
    "biotex_out = \"./corpus_experts/terms_urb/final_terms.csv\"\n",
    "expert_concept = \"./Herelles_ress/termes_graines_urbanisme.txt\"\n",
    "\n",
    "n_first = 10\n",
    "core_biotT_and_expertT(biotex_out, expert_concept, n_first) # data is in dic format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Check if terms already exist in the expert terms\n",
    "expert_concept_risq = './Herelles_ress/termes_graines_natural.dangers.txt'  \n",
    "expert_concept_urb = \"./Herelles_ress/termes_graines_urbanisme.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sim_risq = \"./corpus_experts/terms_risq/final_terms.csv\"\n",
    "no_sim_urb = \"./corpus_experts/terms_urb/final_terms.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_risq = \"./corpus_experts/terms_risq/final_terms_Bert.csv\"\n",
    "sim_urb = \"./corpus_experts/terms_urb/final_terms_Bert.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_concept_risq_list = read_kwd(expert_concept_risq)\n",
    "expert_concept_urb_list = read_kwd(expert_concept_urb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_concept_risq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sim_risq_list = pd.read_csv(no_sim_risq)\n",
    "no_sim_risq_list = no_sim_risq_list['term'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sim_urb_list = pd.read_csv(no_sim_urb)\n",
    "no_sim_urb_list = no_sim_urb_list['term'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_urb_list = pd.read_csv(sim_urb)\n",
    "sim_urb_list = sim_urb_list['term'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_risq_list = pd.read_csv(sim_risq)\n",
    "sim_risq_list = sim_risq_list['term'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csv(some_list, fname):\n",
    "    df = pd.DataFrame(some_list, columns=[\"Termes\"])\n",
    "    df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "final_no_sim_risq_list = [i for i in no_sim_risq_list if i not in expert_concept_risq_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_sim_risq_list = [i for i in final_no_sim_risq_list if i]\n",
    "list_to_csv(final_no_sim_risq_list, './corpus_experts/terms_risq/final_no_sim_risq_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_sim_urb_list = [i for i in no_sim_urb_list if i not in expert_concept_urb_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_sim_urb_list\n",
    "list_to_csv(final_no_sim_urb_list, './corpus_experts/terms_urb/final_no_sim_urb_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\"\n",
    "final_sim_risq_list = [i for i in sim_risq_list if i not in expert_concept_risq_list]\n",
    "final_sim_risq_list\n",
    "list_to_csv(final_sim_risq_list, './corpus_experts/terms_risq/final_sim_risq_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sim_urb_list = [i for i in sim_urb_list if i not in expert_concept_urb_list]\n",
    "final_sim_urb_list\n",
    "list_to_csv(final_sim_urb_list, './corpus_experts/terms_urb/final_sim_urb_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
